{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal: \n",
    "\n",
    "Build a machine-learning algorthim that can predict whether a tweet is more likely to interact with (i.e. retweet at) @BarackObama or @realDonaldTrump, based on the text of the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install --user emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# import emoji\n",
    "# from random import randint\n",
    "import oauth2 as oauth\n",
    "import pandas as pd\n",
    "# import numpy as np\n",
    "from credentials import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oauth_twitter_search(query, consumer_key=CONSUMER_KEY, consumer_secret=CONSUMER_SECRET):\n",
    "    \"\"\" Search Twitter ...\n",
    "    \"\"\"\n",
    "    search_endpoint = \"https://api.twitter.com/1.1/search/tweets.json\"\n",
    "    compiled_search_endpoint = \"{}?q={}+-filter:retweets&count=100&result_type=recent&lang=en&tweet_mode=extended\".format(search_endpoint, query)\n",
    "    consumer = oauth.Consumer(key=CONSUMER_KEY, secret=CONSUMER_SECRET)\n",
    "    client = oauth.Client(consumer)\n",
    "    response, data = client.request(compiled_search_endpoint)\n",
    "    tweets = json.loads(data)\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch the tweets and extract the statuses; \n",
    "tweets_44 = oauth_twitter_search(\"@BarackObama\")['statuses']\n",
    "tweets_45 = oauth_twitter_search(\"@realDonaldTrump\")['statuses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the lists of fetched statuses, and check the count;\n",
    "# we'll get at most 100 recent tweets from each search\n",
    "tweets_all = tweets_44 + tweets_45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prune each fetched tweet set ...\n",
    "# a fetched 'status' is a dictionary. our keys of interest are\n",
    "# the unique id of the tweet and the full text of the tweet\n",
    "pruned_tweets = []\n",
    "for tweet in tweets_all:\n",
    "    d = {}\n",
    "    d['at_44'] = '@barackobama' in tweet['full_text'].lower()\n",
    "    d['at_45'] = '@realdonaldtrump' in tweet['full_text'].lower()\n",
    "    d['id_str'] = tweet['id_str']\n",
    "    d['full_text'] = tweet['full_text']\n",
    "    # since we are trying to predict the presence of such an @mention, \n",
    "    # we remove them from the fetched text\n",
    "    d['pruned_text'] = tweet['full_text'].lower().replace('@barackobama', '').replace('@realdonaldtrump', '')\n",
    "    pruned_tweets.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the data frame from the list of dicts\n",
    "df_tweets = pd.DataFrame(pruned_tweets)\n",
    "# for reasons I don't understand, the order of the keys in each dictionary\n",
    "# does not (always?) transfer to the order of the columns of the data frame,\n",
    "# so we manually correct for this\n",
    "df_tweets = df_tweets[['id_str', 'at_44', 'at_45', 'full_text', 'pruned_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for my curiosity: how many of the fetched tweets are @44 and @45? might be duplicates too\n",
    "df_tweets.loc[(df_tweets['at_44'] == True) & (df_tweets['at_45'] == True)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the csv of previously fetched tweets\n",
    "df = pd.read_csv('44v45tweets.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2677"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take the previously fetched/cleaned tweets and the newly fetched tweets\n",
    "# and put them into a single data frame\n",
    "compiled = pd.concat([df, df_tweets])\n",
    "# how many (possibly non-unique) tweets have we obtained so far?\n",
    "compiled.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we expect duplicates to be fetched, either because (1) a tweet was fetched \n",
    "# by both searches, or (2) it was fetched by two consecutive runs of this script.\n",
    "# so we drop the duplicates\n",
    "compiled.drop_duplicates(subset = ['id_str'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the compiled data frame to csv\n",
    "compiled.to_csv('44v45tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "# how many new tweets (and unique) were fetched?\n",
    "# anecdotal evidence suggests that the rate of tweets that meet (at least)\n",
    "# one of the queries is 10 per minute.\n",
    "# so i've been manually running this notebook every 10-or-so minutes\n",
    "# since 5:30 PDT 4/13/19\n",
    "print(compiled.shape[0] - df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "732"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# of the total fetched tweets, how many are @44 ?\n",
    "compiled.loc[(compiled['at_44'] == True)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2127"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# of the total fetched tweets, how many are @45 ?\n",
    "compiled.loc[(compiled['at_45'] == True)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sat Apr 13 19:40:36 2019'"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "time.ctime(int(time.time()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Progress:\n",
    "\n",
    "As of Sat Apr 13 19:40:36 2019, we have 732 unique tweets @44 and 2127 @45. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
